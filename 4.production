# 1.é…ç½®ç¯å¢ƒ
# å®‰è£…ä¾èµ–ï¼ˆCUDA + CMake + ç¼–è¯‘å·¥å…·ï¼‰
!apt-get update -y
!apt-get install -y cmake build-essential m4 git wget
!apt-get install -y libfftw3-dev libgsl-dev

# æ£€æŸ¥GPUæƒ…å†µ
!nvidia-smi

# å®‰è£… GPU æ”¯æŒç‰ˆ GROMACS
%cd /content
!wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-2022.5.tar.gz
!tar xfz gromacs-2022.5.tar.gz
%cd gromacs-2022.5
!mkdir build
%cd build
!cmake .. -DGMX_GPU=CUDA -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs
!make -j8
!make install

# åŠ è½½ GROMACS ç¯å¢ƒ
!source /usr/local/gromacs/bin/GMXRC
import os
os.environ['PATH'] = '/usr/local/gromacs/bin:' + os.environ['PATH']

!gmx --version

import os

# æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ç¡®è®¤ Google Drive æŒ‚è½½è·¯å¾„æ˜¯å¦æ­£ç¡®
drive_path = "/content/drive/MyDrive/project/md"
if not os.path.exists(drive_path):
    print(f"ç›®å½• {drive_path} ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ...")
    os.makedirs(drive_path)
else:
    print(f"ç›®å½• {drive_path} å­˜åœ¨ã€‚")

# æ£€æŸ¥æ˜¯å¦èƒ½å†™å…¥æ–‡ä»¶
test_file_path = os.path.join(drive_path, "test.txt")
try:
    with open(test_file_path, "w") as test_file:
        test_file.write("Test")
    print("å¯ä»¥æˆåŠŸå†™å…¥æ–‡ä»¶ï¼")
except Exception as e:
    print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

# 2.åˆ›å»ºæ–‡ä»¶
# ä¸Šä¼ charmmguiç”Ÿæˆçš„è½¨è¿¹æ–‡ä»¶åï¼Œæ£€æŸ¥.mdpæ–‡ä»¶
!cat /content/drive/MyDrive/Mproject/md/step5_production.mdp

# æ ¹æ®éœ€è¦ä¿®æ”¹mdpæ–‡ä»¶å‚æ•°ï¼ˆæ­¤å¤„ä¿®æ”¹æ—¶é—´ä¸æ­¥é•¿ï¼‰
# åŸå§‹ MDP æ–‡ä»¶è·¯å¾„
mdp_path = "/content/drive/MyDrive/project/md/step5_production.mdp"
# æ–°çš„ 50ns MDP æ–‡ä»¶è·¯å¾„
new_mdp_path = "/content/drive/MyDrive/project/md/step5_production_50ns.mdp"

# æ–°å‚æ•°è®¾ç½®
new_dt = "0.002"
new_nsteps = "25000000"

# è¯»å–å¹¶ä¿®æ”¹å†…å®¹
with open(mdp_path, "r") as f:
    lines = f.readlines()

new_lines = []
for line in lines:
    if line.strip().startswith("dt"):
        new_lines.append(f"dt                      = {new_dt}\n")
    elif line.strip().startswith("nsteps"):
        new_lines.append(f"nsteps                  = {new_nsteps}\n")
    else:
        new_lines.append(line)

# å†™å…¥æ–°æ–‡ä»¶
with open(new_mdp_path, "w") as f:
    f.writelines(new_lines)

print(f"âœ… å·²ç”Ÿæˆ 50 ns MDP æ–‡ä»¶ï¼š{new_mdp_path}")

# æ£€æŸ¥ç”Ÿæˆçš„.mdpæ–‡ä»¶
# è¯»å– .mdp æ–‡ä»¶å¹¶æå– nsteps å’Œ dt
def parse_mdp(file_path):
    with open(file_path, 'r') as file:
        content = file.readlines()

    dt = None
    nsteps = None

    for line in content:
        if "dt" in line:
            dt = float(line.split()[-1])
        elif "nsteps" in line:
            nsteps = int(line.split()[-1])

    return dt, nsteps

# è®¡ç®—æ¨¡æ‹Ÿæ€»æ—¶é—´
def calculate_total_time(mdp_file_path):
    dt, nsteps = parse_mdp(mdp_file_path)

    if dt is not None and nsteps is not None:
        total_time_ns = dt * nsteps / 1000  # è½¬åŒ–ä¸ºçº³ç§’ï¼ˆnsï¼‰
        return total_time_ns
    else:
        return "æ— æ³•æ‰¾åˆ°å‚æ•° dt æˆ– nsteps"

# ä½¿ç”¨ä½ çš„ .mdp æ–‡ä»¶è·¯å¾„
mdp_file_path = '/content/drive/MyDrive/MD_project_10075026/step5_md_true/step5_production_50ns.mdp'

# è¾“å‡ºæ€»æ—¶é—´
total_time = calculate_total_time(mdp_file_path)
print(f"æ¨¡æ‹Ÿæ€»æ—¶é—´ä¸ºï¼š{total_time} ns")

# 3.ç”Ÿäº§æ¨¡æ‹Ÿ
#  è¿›å…¥å·¥ä½œç›®å½•
%cd /content/drive/MyDrive/project/md

# ç”Ÿäº§æ¨¡æ‹Ÿï¼ˆå¸¦æ–­ç‚¹ç»­è·‘åŠŸèƒ½ï¼Œè‹¥ç»­è·‘å¤±è´¥ï¼Œè¯·åœ¨Google driveä¸­å°†ä½¿ç”¨çš„*.gro *.cpt *.tpr *.log *.xtc *.edrå›é€€åˆ°åŒä¸€ç‰ˆæœ¬é‡æ–°ç»­è·‘ï¼‰
!gmx grompp -f step5_production_50ns.mdp -c npt.gro -t npt.cpt -p topol.top -n index.ndx -o md.tpr
!gmx mdrun -deffnm md -cpi md.cpt -v

# è‡ªåŠ¨å¤‡ä»½å…³é”®è¾“å‡ºæ–‡ä»¶
!mkdir -p /content/drive/MyDrive/project/md_backup
!cp *.gro *.cpt *.tpr *.log *.xtc *.edr /content/drive/MyDrive/project/md_backup/

# 4. æ•°æ®åˆ†æ
# è¿›å…¥å·¥ä½œç›®å½•
%cd /content/drive/MyDrive/project/md

# æ¸©åº¦ï¼Œå‹åŠ›ï¼Œæ€»èƒ½é‡ï¼Œrmsdï¼Œrgå¯è§†åŒ–
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import os

# ğŸ“ è®¾ç½®å·¥ä½œç›®å½•ï¼ˆæ ¹æ®ä½ çš„å®é™…æƒ…å†µï¼‰
work_dir = '/content/drive/MyDrive/MD_project_10075026/step5_md_1026'
os.chdir(work_dir)

# ğŸ¨ å…¨å±€å­—ä½“ä¸å­—å·è®¾ç½®
mpl.rcParams['font.family'] = 'Arial'
mpl.rcParams['font.size'] = 12
mpl.rcParams['axes.titlesize'] = 14
mpl.rcParams['axes.labelsize'] = 12
mpl.rcParams['xtick.labelsize'] = 10
mpl.rcParams['ytick.labelsize'] = 10

# ç»Ÿä¸€ç»˜å›¾å‡½æ•°ï¼Œæ–°å¢ xunit å‚æ•°ï¼ˆé»˜è®¤ 'ps'ï¼‰
def plot_xvg(file, title, ylabel, save_as, xunit='ps'):
    time = []
    value = []
    with open(file, 'r') as f:
        for line in f:
            if line.startswith(('@', '#')):
                continue
            cols = line.strip().split()
            time.append(float(cols[0]))
            value.append(float(cols[1]))

    plt.figure(figsize=(8,5))
    plt.plot(time, value, color='#2174AE')
    plt.title(title)
    plt.xlabel(f'Time ({xunit})')
    plt.ylabel(ylabel)
    plt.grid(False)
    plt.tight_layout()
    plt.savefig(save_as.replace('.png', '.tif'), dpi=900)
    plt.show()
    print(f"âœ… å·²ä¿å­˜å›¾è¡¨ï¼š{save_as}")

# ğŸ› ï¸ ç”Ÿæˆ xvg æ•°æ®ï¼ˆè·Ÿä½ ä¹‹å‰å®Œå…¨ä¸€è‡´ï¼‰
!echo 16  | gmx energy -f md.edr -o temperature.xvg     # Temperature
!echo 18  | gmx energy -f md.edr -o pressure.xvg        # Pressure
!echo 10  | gmx energy -f md.edr -o total_energy.xvg    # Total Energy
!echo 4 4  | gmx rms    -s md.tpr -f md.xtc -o rmsd.xvg -tu ns
!gmx gyrate -s md.tpr -f md.xtc -o rg.xvg

# ğŸ“Š ç»˜å›¾æ—¶æ˜¾å¼æŒ‡å®š rmsd ç”¨ 'ns'
plot_xvg('temperature.xvg',  'Temperature Over Time',      'Temperature (K)',   'temperature.png',   xunit='ps')
plot_xvg('pressure.xvg',     'Pressure Over Time',         'Pressure (bar)',     'pressure.png',      xunit='ps')
plot_xvg('total_energy.xvg', 'Total Energy Over Time',     'Energy (kJ/mol)',    'total_energy.png',  xunit='ps')
plot_xvg('rmsd.xvg',         'RMSD Over Time',             'RMSD (nm)',          'rmsd.png',          xunit='ns')
plot_xvg('rg.xvg',           'Radius of Gyration Over Time','Rg (nm)',            'rg.png',            xunit='ps')

#æ£€æŸ¥èƒ½é‡è·³ç‚¹å¹¶å¯è§†åŒ–
# è®¾ç½®å·¥ä½œç›®å½•ï¼ˆæ ¹æ®ä½ çš„å®é™…æƒ…å†µï¼‰
work_dir = '/content/drive/MyDrive/project/md'
os.chdir(work_dir)

import subprocess, sys, os
import numpy as np, pandas as pd

EDR = "md.edr"   # å¦‚æœä¸æ˜¯è¿™ä¸ªåå­—æ”¹è¿‡æ¥
TMPXVG = "total.xvg"
LOG = "md.log"
# è¿™é‡Œçš„ index_num æ˜¯ gmx energy è¾“å‡ºé‡Œçš„ç¼–å·ï¼Œä»£è¡¨ Total Energy
# å…ˆäº¤äº’ç¡®è®¤ä¸€ä¸‹ä½ çš„ Total Energy åœ¨ gmx energy ä¸­æ˜¯ç¬¬å‡ é¡¹ï¼Œç„¶åè®¾ç½®è¿™ä¸ªå€¼
index_num = 13   # <<-- **æ ¹æ®å®é™…äº¤äº’ç»“æœä¿®æ”¹**ï¼Œå¸¸è§ä¸æ˜¯10æ—¶è¯·æ›¿æ¢

# 1) è°ƒç”¨ gmx energy éäº¤äº’å¯¼å‡º total energy
try:
    p = subprocess.run(["gmx", "energy", "-f", EDR, "-o", TMPXVG],
                       input=f"{index_num}\n".encode(),
                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
except subprocess.CalledProcessError as e:
    print("gmx energy è°ƒç”¨å¤±è´¥ï¼š", e.stderr.decode()[:1000])
    raise

print("gmx energy å¯¼å‡ºå®Œæˆ ->", TMPXVG)

# 2) è¯»å– xvg å¹¶æ£€æµ‹è·³ç‚¹
def read_xvg(path):
    with open(path) as f:
        lines=[l for l in f if not l.startswith(('#','@'))]
    from io import StringIO
    df=pd.read_csv(StringIO(''.join(lines)), sep=r'\s+', header=None)
    return df

df = read_xvg(TMPXVG)
t = df.iloc[:,0].values
E = df.iloc[:,1].values
d = np.diff(E)
thr = max(1000, np.std(d)*8)   # é˜ˆå€¼ï¼šè‡³å°‘ 1000 kJ/mol æˆ– 8*sigma
jumps = np.where(np.abs(d) > thr)[0]

print("threshold:", thr)
if len(jumps) == 0:
    print("No large jumps detected in total energy.")
else:
    print("Detected jumps (index of frame before jump):", jumps.tolist())
    for j in jumps:
        print(f" jump: {t[j]} ps -> {t[j+1]} ps   delta = {E[j+1]-E[j]:.1f} kJ/mol")

# 3) æ‰“å° md.log ä¸­ä¸ checkpoint/restart/Starting mdrun ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼ˆè‹¥æœ‰ï¼‰
if os.path.exists(LOG):
    with open(LOG) as f:
        lines = f.readlines()
    keywords = ["Restarting from checkpoint", "Reading checkpoint", "Starting mdrun", "Will start from checkpoint"]
    print("\n---- md.log keyword occurrences (with +/-5 lines context) ----")
    for i, line in enumerate(lines):
        for kw in keywords:
            if kw in line:
                start = max(0, i-5)
                end = min(len(lines), i+6)
                print(f"\n--- Context around line {i+1}: (keyword: {kw}) ---")
                print(''.join(lines[start:end]))
else:
    print("md.log not found in current directory.")
import numpy as np
import matplotlib.pyplot as plt

#  è¯»å–æ•°æ® 
file = 'total.xvg'  # ä½ çš„èƒ½é‡æ–‡ä»¶å
data = np.loadtxt(file, comments=['#', '@'])
time = data[:,0]
energy = data[:,1]

# è®¡ç®—èƒ½é‡å˜åŒ–
diff = np.diff(energy)
abs_diff = np.abs(diff)

# è®¾å®šæ£€æµ‹é˜ˆå€¼
threshold = np.mean(abs_diff) + 5 * np.std(abs_diff)
jumps = np.where(abs_diff > threshold)[0]

print(f"æ£€æµ‹åˆ°è·³ç‚¹æ•°é‡: {len(jumps)}")
for j in jumps:
    print(f"jump {time[j]:.2f} â†’ {time[j+1]:.2f} ps, Î”E = {energy[j+1]-energy[j]:.1f} kJ/mol")

# ç»˜å›¾
plt.figure(figsize=(10,5))
plt.plot(time, energy, lw=0.8, label='Total Energy')
if len(jumps) > 0:
    plt.scatter(time[jumps], energy[jumps], color='red', label='Detected Jump')
plt.xlabel('Time (ps)')
plt.ylabel('Total Energy (kJ/mol)')
plt.title('Total Energy vs Time')
plt.legend()
plt.tight_layout()
plt.show()

# sasaåˆ†æ
# è¿è¡Œå‰ç¡®è®¤ï¼šref_renamed.pdb å’Œ aligned.xtc åœ¨å½“å‰ç›®å½•æˆ–æŒ‡å®šè·¯å¾„
# å¦‚æœæ–‡ä»¶åœ¨ Driveï¼Œå…ˆ drive.mount(...) å¹¶æŠŠè·¯å¾„æ”¹æˆ /content/drive/...
work_dir = '/content/drive/MyDrive/project/md/'
%cd $work_dir

import os, sys, math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

import MDAnalysis as mda
from MDAnalysis.lib import distances
from MDAnalysis.analysis import contacts
from MDAnalysis.analysis import rms

# 1) å‚æ•°è®¾å®šï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
pdb_ref = "ref_renamed.pdb"      # ä½ çš„å‚è€ƒ PDBï¼ˆå·²æ›¿æ¢ segidï¼‰
traj_tpr = "md.tpr"              # å¯é€‰ï¼Œå¦‚æœä¸éœ€è¦å†è½½å…¥æ‹“æ‰‘å¯ç½® None
traj_xtc = "aligned.xtc"         # å·²å¯¹é½çš„è½¨è¿¹ï¼ˆéå¸¸é‡è¦ï¼‰

# å¦‚æœä½ æ˜¯ç”¨ md.tpr + aligned.xtcï¼Œåˆ™ universe = Universe(md.tpr, aligned.xtc)
# å¦‚æœåªæœ‰ pdb + aligned.xtcï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Universe(pdb, aligned.xtc)
chainA = "seg_1_PROB"            # è½¨è¿¹ä¸­ä»£è¡¨é…¶/è›‹ç™½1çš„ segidï¼ˆæŒ‰ä½ ç¡®è®¤ï¼‰
chainB = "seg_2_PROC"            # è½¨è¿¹ä¸­ä»£è¡¨è›‹ç™½2çš„ segid
contact_cutoff = 0.4             # nm (0.4 nm = 4 Ã…)
water_cutoff = 0.35              # nmï¼Œæ°´æ¥è§¦ç•Œé¢é˜ˆå€¼
step = 1                         # è½¨è¿¹æ­¥é•¿ï¼ˆé‡‡æ ·ï¼‰ï¼Œå¯æ”¹ä¸º 5/10 ä»¥åŠ é€Ÿ
out_dir = "ppi_results"
os.makedirs(out_dir, exist_ok=True)

# 2) è½½å…¥ Universe
# prefer tpr+xtc if available (keeps topology)
if os.path.exists(traj_tpr) and os.path.exists(traj_xtc):
    u = mda.Universe(traj_tpr, traj_xtc)
else:
    u = mda.Universe(pdb_ref, traj_xtc)

ref = mda.Universe(pdb_ref)  # static reference for SASA or checks

# 3) selections
A_atoms = u.select_atoms(f"segid {chainA} and protein")
B_atoms = u.select_atoms(f"segid {chainB} and protein")
print(f"A atoms: {len(A_atoms)}, residues: {len(A_atoms.residues)}")
print(f"B atoms: {len(B_atoms)}, residues: {len(B_atoms.residues)}")

# residue lists (ordered)
A_residues = list(A_atoms.residues)
B_residues = list(B_atoms.residues)
nA = len(A_residues); nB = len(B_residues)

# create index maps for residues
A_idx_map = {r.resid: i for i,r in enumerate(A_residues)}
B_idx_map = {r.resid: j for j,r in enumerate(B_residues)}

# 4) Compute residue-residue contact occupancy
#    We'll compute for sampled frames: for each residue pair (i,j) whether any atom-atom distance < cutoff
#    This is O(n_pairs * n_frames * atoms_per_residue^2) but acceptable for ~70x70 residues.

frames = list(range(0, len(u.trajectory), step))
nframes = len(frames)
print("Frames sampled:", nframes)

# initialize counters
pair_counts = np.zeros((nA, nB), dtype=int)   # count frames where residue pair is in contact
A_contact_counts = np.zeros(nA, dtype=int)    # per-residue frames count contacting other chain
B_contact_counts = np.zeros(nB, dtype=int)
water_counts_per_frame = np.zeros(nframes, dtype=int)

# pre-build per-residue atom index lists for speed (global atom indices)
A_res_atom_inds = [res.atoms.indices for res in A_residues]
B_res_atom_inds = [res.atoms.indices for res in B_residues]

# water selection (adjust resname if your water name differs)
# common names: TIP3, SOL, W, HOH
water_sel = u.select_atoms("resname TIP3 or resname SOL or resname W or resname HOH")
print("Water atoms:", len(water_sel), "water residues:", len(water_sel.residues))

# interface residue mask (will be filled after computing pairwise contacts)
interface_mask_A = np.zeros(nA, dtype=bool)
interface_mask_B = np.zeros(nB, dtype=bool)

pbar = tqdm(total=nframes, desc="Frames")
for idx_frame, frame in enumerate(frames):
    u.trajectory[frame]  # jump to frame
    # coordinates arrays
    coords = u.atoms.positions  # in Ã…; distances functions accept Ã… (MDAnalysis default)
    # But our cutoffs are in nm; convert to Ã…
    cutoff_ang = contact_cutoff * 10.0
    # For speed: compute full distance array between atoms in A and B? That could be large.
    # We'll loop residue pairs (nA * nB manageable for ~70x70)
    for i in range(nA):
        ai_inds = A_res_atom_inds[i]
        ai_pos = coords[ai_inds]
        for j in range(nB):
            bj_inds = B_res_atom_inds[j]
            bj_pos = coords[bj_inds]
            # compute minimal distance between ai_pos and bj_pos
            # use numpy broadcasting if small
            d = distances.distance_array(ai_pos, bj_pos, box=u.dimensions)
            if d.size == 0:
                continue
            if d.min() <= cutoff_ang:
                pair_counts[i,j] += 1
                A_contact_counts[i] += 1
                B_contact_counts[j] += 1
                interface_mask_A[i] = True
                interface_mask_B[j] = True
    # water count near interface atoms: first get interface atoms (current frame)
    if interface_mask_A.any() or interface_mask_B.any():
        # build interface atom indices for this frame (all atoms of residues currently contacting)
        interface_atoms_inds = np.concatenate([A_res_atom_inds[i] for i in np.where(interface_mask_A)[0]] +
                                             [B_res_atom_inds[j] for j in np.where(interface_mask_B)[0]])
        if interface_atoms_inds.size>0:
            # unique water residues within water_cutoff (nm => Ã…)
            wc_ang = water_cutoff * 10.0
            # get water residue centers or atom positions
            # Using distance_array between water oxygen atoms and interface atoms is efficient
            water_ox = water_sel.select_atoms("name O or name OW or name OH2")
            if len(water_ox)>0 and len(interface_atoms_inds)>0:
                d_wo = distances.distance_array(u.atoms.positions[water_ox.indices],
                                               u.atoms.positions[interface_atoms_inds], box=u.dimensions)
                close_water_inds = np.where(d_wo.min(axis=1) <= wc_ang)[0]
                # unique water residues
                water_resids = {water_ox.residues[i].resid for i in close_water_inds}
                water_counts_per_frame[idx_frame] = len(water_resids)
            else:
                water_counts_per_frame[idx_frame] = 0
    pbar.update(1)
pbar.close()

# 5) Normalize counts -> occupancy (fraction of sampled frames)
pair_occupancy = pair_counts / float(nframes)    # shape nA x nB
A_occ = (pair_counts.sum(axis=1) > 0).astype(int)  # boolean whether residue ever contacts
# but more useful: per-residue contact fraction (fraction of frames residue i contacts ANY j)
A_contact_fraction = np.zeros(nA)
B_contact_fraction = np.zeros(nB)
# for each residue, count frames where it had at least one contact
# we computed A_contact_counts as incremented per pair; but that overcounts frames when residue contacts multiple partners.
# So we recompute properly:
# Recompute by iterating frames again but only tracking per-residue presence might be expensive;
# Instead approximate: assume earlier method added once per pair per frame; we will compute proper per-residue presence by:
# quick recompute: for each frame we can check min distance to other chain per residue â€” do that now but using step frames (we already sampled)
A_frame_presence = np.zeros((nframes, nA), dtype=bool)
B_frame_presence = np.zeros((nframes, nB), dtype=bool)

pbar = tqdm(total=nframes, desc="Recomputing per-residue presence")
for idx_frame, frame in enumerate(frames):
    u.trajectory[frame]
    coords = u.atoms.positions
    cutoff_ang = contact_cutoff * 10.0
    for i in range(nA):
        ai_pos = coords[A_res_atom_inds[i]]
        # compute min dist to entire B atoms (faster than pairwise)
        d = distances.distance_array(ai_pos, coords[np.concatenate(B_res_atom_inds)], box=u.dimensions)
        if d.size>0 and d.min() <= cutoff_ang:
            A_frame_presence[idx_frame, i] = True
    for j in range(nB):
        bj_pos = coords[B_res_atom_inds[j]]
        d = distances.distance_array(bj_pos, coords[np.concatenate(A_res_atom_inds)], box=u.dimensions)
        if d.size>0 and d.min() <= cutoff_ang:
            B_frame_presence[idx_frame, j] = True
    pbar.update(1)
pbar.close()

A_contact_fraction = A_frame_presence.sum(axis=0) / float(nframes)
B_contact_fraction = B_frame_presence.sum(axis=0) / float(nframes)

# 6) Save residue-level results
# Build residue labels
A_labels = [f"{res.resname}{res.resid}" for res in A_residues]
B_labels = [f"{res.resname}{res.resid}" for res in B_residues]

df_A = pd.DataFrame({"res_label": A_labels, "contact_fraction": A_contact_fraction})
df_B = pd.DataFrame({"res_label": B_labels, "contact_fraction": B_contact_fraction})

df_A.to_csv(os.path.join(out_dir, "residue_contact_fraction_A.csv"), index=False)
df_B.to_csv(os.path.join(out_dir, "residue_contact_fraction_B.csv"), index=False)

# pair matrix -> DataFrame
df_pair = pd.DataFrame(pair_occupancy, index=A_labels, columns=B_labels)
df_pair.to_csv(os.path.join(out_dir, "residue_residue_contact_occupancy.csv"))

# water counts per frame
df_water = pd.DataFrame({"frame_index": frames, "water_count": water_counts_per_frame})
df_water.to_csv(os.path.join(out_dir, "interface_water_counts.csv"), index=False)

print("Saved CSVs to", out_dir)

pip install freesasa

import MDAnalysis as mda
import numpy as np
import pandas as pd
import os

# é…ç½®
pdb_ref = "ref_renamed.pdb"   # ä¿®æ”¹ä¸ºä½ çš„å¤åˆç‰©PDBè·¯å¾„
output_csv = "sasa_per_residue.csv"

try:
    from MDAnalysis.analysis.sasa import ShrakeRupley
    print("âœ… ä½¿ç”¨ MDAnalysis ShrakeRupley è¿›è¡Œ SASA è®¡ç®—")

    u = mda.Universe(pdb_ref)
    sr = ShrakeRupley(probe_radius=1.4, n_sphere_points=960)
    sr.run(u)
    sasa_res = [res.sasa for res in u.residues]

    df = pd.DataFrame({
        "chain": [res.segid for res in u.residues],
        "resname": [res.resname for res in u.residues],
        "resid": [res.resid for res in u.residues],
        "sasa": sasa_res
    })
    df.to_csv(output_csv, index=False)
    print(f"âœ… SASAè®¡ç®—å®Œæˆï¼ˆMDAnalysisï¼‰ï¼Œç»“æœå·²ä¿å­˜ï¼š{output_csv}")

except Exception as e:
    print(f"âš ï¸ MDAnalysis SASAä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°FreeSASAã€‚åŸå› : {e}")
    try:
        import freesasa

        print("âœ… ä½¿ç”¨ FreeSASA è¿›è¡Œ SASA è®¡ç®—")
        structure = freesasa.Structure(pdb_ref)
        result = freesasa.calc(structure)

        # ----------- æ£€æµ‹æ¥å£ç‰ˆæœ¬ -----------
        if hasattr(freesasa, "residueAreas"):
            # æ—§æ¥å£ï¼ˆéƒ¨åˆ†ç³»ç»Ÿä»ä¿ç•™ï¼‰
            res_areas = freesasa.residueAreas(structure, result)
            data = []
            for chain_id in res_areas:
                for res_name in res_areas[chain_id]:
                    for res_num in res_areas[chain_id][res_name]:
                        sasa_val = res_areas[chain_id][res_name][res_num].total
                        data.append({
                            "chain": chain_id,
                            "resname": res_name,
                            "resid": res_num,
                            "sasa": sasa_val
                        })
            df = pd.DataFrame(data)

        else:
            # æ–°æ¥å£ï¼šé€šè¿‡classifierè§£æ
            data = []
            for i in range(structure.nAtoms()):
                atom_area = result.atomArea(i)
                resnum = structure.residueNumber(i)
                resname = structure.residueName(i)
                chain = structure.chainLabel(i)
                data.append((chain, resname, resnum, atom_area))
            df_tmp = pd.DataFrame(data, columns=["chain", "resname", "resid", "atom_sasa"])
            df = df_tmp.groupby(["chain", "resname", "resid"], as_index=False)["atom_sasa"].sum()
            df.rename(columns={"atom_sasa": "sasa"}, inplace=True)

        df.to_csv(output_csv, index=False)
        print(f"âœ… SASAè®¡ç®—å®Œæˆï¼ˆFreeSASAï¼‰ï¼Œç»“æœå·²ä¿å­˜ï¼š{output_csv}")

    except Exception as e2:
        print(f"âŒ FreeSASAè®¡ç®—å¤±è´¥ï¼š{e2}")
        print("è¯·ç¡®è®¤ï¼š")
        print(" - FreeSASA æ˜¯å¦æ­£ç¡®å®‰è£…ï¼ˆpip install freesasaï¼‰")
        print(" - è¾“å…¥çš„ PDB æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼ˆå«åŸå­åæ ‡å’Œé“¾ä¿¡æ¯ï¼‰")
# å¯è§†åŒ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

# è¯»å–è¾“å‡ºæ–‡ä»¶
sasa = pd.read_csv("sasa_per_residue.csv")
contA = pd.read_csv("residue_contact_fraction_A.csv")
contB = pd.read_csv("residue_contact_fraction_B.csv")
water = pd.read_csv("interface_water_counts.csv")

print("âœ… æ–‡ä»¶è¯»å–æˆåŠŸ")

# é¢„å¤„ç†
# ç»Ÿä¸€æ®‹åŸºæ ‡è¯†
sasa["residue"] = sasa["chain"].astype(str) + "-" + sasa["resname"].astype(str) + sasa["resid"].astype(str)
contA["residue"] = contA["res_label"].astype(str)
contB["residue"] = contB["res_label"].astype(str)

# é€‰å–æœ‰ç”¨åˆ—
sasa = sasa[["residue", "sasa"]]
contA = contA[["residue", "contact_fraction"]].rename(columns={"contact_fraction": "contact_A"})
contB = contB[["residue", "contact_fraction"]].rename(columns={"contact_fraction": "contact_B"})

# åˆå¹¶è¡¨æ ¼
df = sasa.merge(contA, on="residue", how="outer")
df = df.merge(contB, on="residue", how="outer")
df.fillna(0, inplace=True)

# è®¡ç®—å¹³å‡æ¥è§¦å æœ‰ç‡
df["contact_total"] = df[["contact_A", "contact_B"]].mean(axis=1)

# è®¡ç®—å¹³å‡æ°´åˆ†å­æ•°ï¼ˆæ•´ä½“ï¼‰
mean_water = water["water_count"].mean()
print(f"ğŸ’§ å¹³å‡ç•Œé¢æ°´åˆ†å­æ•°: {mean_water:.1f}")

#  ç»˜åˆ¶ SASA vs Contact
plt.figure(figsize=(10, 6), dpi=300)
sns.set(style="whitegrid", font_scale=1.2)
plt.hexbin(df["sasa"], df["contact_total"], gridsize=30, cmap="viridis", bins='log')

# æ ‡æ³¨å…³é”®æ®‹åŸºï¼ˆå‰5%ï¼‰
threshold_x = np.percentile(df["sasa"], 95)
threshold_y = np.percentile(df["contact_total"], 95)
key_res = df[(df["sasa"] > threshold_x) | (df["contact_total"] > threshold_y)]

for _, row in key_res.iterrows():
    plt.text(row["sasa"], row["contact_total"], row["residue"],
             fontsize=7, fontweight="bold", color="crimson")

plt.xlabel("Solvent Accessible Surface Area (Ã…Â²)", fontsize=12)
plt.ylabel("Residue Contact Fraction (A+B avg)", fontsize=12)
plt.title("Residue SASA vs Contact Density Map", fontsize=14, weight="bold")
plt.colorbar(label="Residue Density (log scale)")
plt.tight_layout()
plt.savefig("SASA_vs_ContactDensity.tif", format="tif", dpi=900)
plt.show()
files.download("SASA_vs_ContactDensity.tif")

# ç»˜åˆ¶ SASA åˆ†å¸ƒ + å¹³å‡æ°´å«é‡èƒŒæ™¯
plt.figure(figsize=(10, 6), dpi=300)
sns.kdeplot(df["sasa"], fill=True, color="skyblue", alpha=0.6)
plt.axvline(df["sasa"].mean(), color="red", linestyle="--", label=f"Mean SASA = {df['sasa'].mean():.1f}")
plt.axhline(mean_water, color="green", linestyle=":", label=f"Mean Interface Water = {mean_water:.1f}")
plt.xlabel("Residue SASA (Ã…Â²)")
plt.ylabel("Density")
plt.title("Residue SASA Distribution with Water-Level Context", weight="bold")
plt.legend()
plt.tight_layout()
plt.savefig("SASA_Distribution_WaterContext.tif", format="tif", dpi=900)
plt.show()
files.download("SASA_Distribution_WaterContext.tif")

# å¯¼å‡ºå…³é”®æ®‹åŸºè¡¨
key_res.to_csv("Top_residues.csv", index=False)
files.download("Top_residues.csv")

print("âœ… ç»˜å›¾å®Œæˆï¼å·²å¯¼å‡ºé«˜åˆ†è¾¨ç‡ .tif å›¾ä¸å…³é”®æ®‹åŸºè¡¨ã€‚")

# 6.æ´»æ€§ä½ç‚¹å¯è§†åŒ–
import pandas as pd
from google.colab import files

# ========= è¾“å…¥æ–‡ä»¶ =========
contactA_file = "/content/drive/MyDrive/project/md/ppi_results/residue_contact_fraction_A.csv"
contactB_file = "/content/drive/MyDrive/project/md/ppi_results/residue_contact_fraction_B.csv"
output_cxc = "highlight_residues_chimerax.cxc"
chain_id = "A"  # æ”¹æˆä½ çš„é“¾

# è¯»å–CSVå¹¶å¤„ç†
contA = pd.read_csv(contactA_file)
contB = pd.read_csv(contactB_file)

for df in [contA, contB]:
    if "res_label" in df.columns:
        df["residue"] = df["res_label"]
    elif {"resname", "resid"}.issubset(df.columns):
        df["residue"] = df["resname"].astype(str) + df["resid"].astype(str)
    else:
        raise ValueError("âŒ æœªæ‰¾åˆ°res_labelæˆ–(resname,resid)åˆ—")

merged = pd.merge(contA[["residue", "contact_fraction"]],
                  contB[["residue", "contact_fraction"]],
                  on="residue", how="outer", suffixes=("_A", "_B"))
merged.fillna(0, inplace=True)
merged["contact_avg"] = merged[["contact_fraction_A", "contact_fraction_B"]].mean(axis=1)

# å–å‰15ä¸ªé«˜æ¥è§¦æ®‹åŸº
top = merged.sort_values("contact_avg", ascending=False).head(15)

# ç”Ÿæˆé¢œè‰²è¡¨ï¼ˆå¯æŒ‰éœ€è¦ä¿®æ”¹ï¼‰
colors = ["red", "blue", "green", "orange", "purple", "cyan", "magenta", "yellow", "salmon", "lime",
          "pink", "gold", "violet", "turquoise", "brown"]

# ç”Ÿæˆ ChimeraX è„šæœ¬
lines = [
    "# ChimeraX Highlight Residues Script",
    "# æ‰“å¼€æ¨¡å‹åç›´æ¥è¿è¡Œæœ¬è„šæœ¬",
    "",
    "color white",
    "show",
    "lighting soft",
]

for i, (_, row) in enumerate(top.iterrows()):
    label = row["residue"]
    resname = ''.join([c for c in label if c.isalpha()])[:3].upper()
    resid = ''.join([c for c in label if c.isdigit()])
    contact = row['contact_avg']
    color = colors[i % len(colors)]
    lines.append(f"# {resname}{resid} å¹³å‡æ¥è§¦å æœ‰ç‡ {contact:.3f}")
    # è®¾ç½®æ£’çŠ¶ç»“æ„
    lines.append(f"style stick :{resid}.{chain_id}")
    # è®¾ç½®é¢œè‰²
    lines.append(f"color {color} :{resid}.{chain_id}")
    # æ·»åŠ æ ‡æ³¨
    lines.append(f'label create :{resid}.{chain_id} text "{resname}{resid}" size 1.2 color {color}')
    lines.append("")

lines.append("view")

# ä¿å­˜æ–‡ä»¶
with open(output_cxc, "w", encoding="utf-8", newline="\n") as f:
    f.write("\n".join(lines))

print("âœ… å·²ç”Ÿæˆ ChimeraX è„šæœ¬ï¼Œå¯ç›´æ¥æ‰“å¼€:", output_cxc)
files.download(output_cxc)
